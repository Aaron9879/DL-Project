{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEHyseHxA8q4"
   },
   "source": [
    "# <font color=\"#0b486b\">  FIT5215: Deep Learning (2024) - Assignment 2 (Main Part)</font>\n",
    "***\n",
    "*CE/Lecturer (Clayton):*  **Dr Trung Le** | trunglm@monash.edu <br/>\n",
    "*Lecturer (Clayton):* **Prof Dinh Phung** | dinh.phung@monash.edu <br/>\n",
    "*Lecturer (Malaysia):*  **Dr Arghya Pal** | arghya.pal@monash.edu <br/>\n",
    "*Lecturer (Malaysia):*  **Dr Lim Chern Hong** | lim.chernhong@monash.edu <br/>  <br/>\n",
    "*Head Tutor 3181:*  **Miss Vy Vo** |  \\[v.vo@monash.edu \\] <br/>\n",
    "*Head Tutor 5215:*  **Dr Van Nguyen** |  \\[van.nguyen1@monash.edu \\]\n",
    "\n",
    "<br/> <br/>\n",
    "Faculty of Information Technology, Monash University, Australia\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqMi8gdDBD1L"
   },
   "source": [
    "# <font color=\"#0b486b\">  Student Information</font>\n",
    "***\n",
    "Surname: **Peng**  <br/>\n",
    "Firstname: **Yuhang**    <br/>\n",
    "Student ID: **34278818**    <br/>\n",
    "Email: **ypen0076@student.monash.edu**    <br/>\n",
    "Your tutorial time: **LAB 05 MON 16:00**    <br/>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e8EwkVtGva7"
   },
   "source": [
    "# <font color=\"0b486b\">Assignment 2 – Deep Learning for Sequential Data</font>\n",
    "### Due: <font color=\"red\">11:55pm Sunday, 20 October 2024</font> (FIT5215)\n",
    "\n",
    "#### <font color=\"red\">Important note:</font> This is an **individual** assignment. It contributes **20%** to your final mark. Read the assignment instructions carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PF8vqRzTCEsm"
   },
   "source": [
    "## <font color=\"#0b486b\">Assignment 2's Organization</font>\n",
    "This assignment 2 has two (2) sections:\n",
    "- Section 1: Fundamentals of RNNs (10 marks).\n",
    "- Section 2: Deep Learning for Sequential Data (90 marks). This section is further divided into 4 parts.\n",
    "\n",
    "The assignment 2 is organized in three (3) notebooks.\n",
    "- Notebook 1 (this notebook) [Total: 30 marks] includes Section 1 as well as Part 1 and Part 2 of Section 2.\n",
    "- Notebook 2 ([link](https://colab.research.google.com/drive/1m0mh9Mk4-AKEhgAHRwQdl5mc0x7SF7Tv?usp=sharing)) [Total: 40 marks] includes Part 3 of Section 2.\n",
    "- Notebook 3 ([link](https://colab.research.google.com/drive/1JfMZeCkkvjZ5LvKNV-UnR10pl-RogMgF?usp=sharing)) [Total: 30 marks] includes Part 4 of Section 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WnlGo-hGBvlO"
   },
   "source": [
    "## <font color=\"#0b486b\">What to submit</font>\n",
    "\n",
    "This assignment is to be completed individually and submitted to Moodle unit site. **By the due date, you are required to submit one  <font color=\"red; font-weight:bold\">single zip file, named xxx_assignment02_solution.zip</font> where `xxx` is your student ID, to the corresponding Assignment (Dropbox) in Moodle**. You can use Google Colab to do Assignment 2 but you need to save it to an `*.ipynb` file to submit to the unit Moodle.\n",
    "\n",
    "**More importantly, if you use Google Colab to do this assignment, you need to first make a copy of this notebook on your Google drive**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVGeGjkzn4RG"
   },
   "source": [
    "***For example, if your student ID is <font color=\"red; font-weight:bold\">12356</font>, then gather all of your assignment solutions to a folder, create a zip file named <font color=\"red; font-weight:bold\">123456_assignment02_solution.zip</font> and submit this file.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3J6gGAMn44Z"
   },
   "source": [
    "Within this zip folder, you **must** submit the following files <u>for each part</u>:\n",
    "1.\t**`FIT5215_DeepLearning_Assignment2_Official[Main].ipynb`**:  this is your Python notebook solution source file.\n",
    "1.\t**`FIT5215_DeepLearning_Assignment2_Official[Main].html`**: this is the output of your Python notebook solution *exported* in HTML format.\n",
    "1. **`FIT5215_DeepLearning_Assignment2_Official[RNNs].ipynb`**\n",
    "1. **`FIT5215_DeepLearning_Assignment2_Official[RNNs].html`**\n",
    "1. **`FIT5215_DeepLearning_Assignment2_Official[Transformers].ipynb`**\n",
    "1. **`FIT5215_DeepLearning_Assignment2_Official[Transformers].html`**\n",
    "1.\tAny **extra files or folder** needed to complete your assignment (e.g., images used in your answers).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LsB3OmMxB4Dh"
   },
   "source": [
    "## Section 1: Fundamentals in RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c83ea5R9nh_y"
   },
   "source": [
    "You need to **manually** implement a multi-timestep Recurrent Neural Network that can take an input as a 3D tensor `[batch_size, seq_len, input_size]` for a classification task.\n",
    "\n",
    "<div style=\"text-align: right\"><font color=\"red\">[Total: 10 marks]</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OzH6RTIjExy2"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKlCx1D3rczS"
   },
   "source": [
    "We declare the relevant variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "C77pPiGBA9IJ"
   },
   "outputs": [],
   "source": [
    "input_size = 5\n",
    "seq_len = 4\n",
    "batch_size = 8\n",
    "hidden_size = 3\n",
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A10tCeyKrkUt"
   },
   "source": [
    "We create random inputs (i.e., `inputs`) and random labels (i.e., `random_labels`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ybfqj-wBEu0H",
    "outputId": "0387f520-f380-47e0-b7e2-c0107d18d77b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 5])\n",
      "tensor([2, 1, 1, 1, 1, 0, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.randn(batch_size, seq_len, input_size)\n",
    "random_labels = torch.randint(0, num_classes, (batch_size,))\n",
    "print(inputs.shape)\n",
    "print(random_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7ABTPhyr6wa"
   },
   "source": [
    "(1) In what follows, we need to declare the model parameters, which include the matrices $U$ (``[input_size, hidden_size]``), W (``[hidden_size, hidden_size]``), $V$ (``[hidden_size, num_classes]``) and the biases $b$ and $c$ for the hidden states and logits respectively.\n",
    "\n",
    "<div style=\"text-align: right\"><font color=\"red\">[2 marks]</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qzlZtACzsbSR"
   },
   "outputs": [],
   "source": [
    "#Insert your code here\n",
    "U = torch.randn(input_size, hidden_size, requires_grad=True)\n",
    "W = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
    "b = torch.randn(hidden_size, requires_grad=True)\n",
    "V = torch.randn(hidden_size, num_classes, requires_grad=True)\n",
    "c = torch.randn(num_classes, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MW_gal7Us9fK"
   },
   "source": [
    "(2) Next you need to write the code to compute `hiddens` which is a 3D tensor of the shape ``[batch_size, seq_len, hidden_size]`` using the formula of the simple/standard RNN cells. You can freely modify the code below.\n",
    "\n",
    "<div style=\"text-align: right\"><font color=\"red\">[2 marks]</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h_t = \\text{tanh}(x_t \\cdot U + h_{t-1} \\cdot W + b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "KwxqdXkTtaea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.8330,  0.9466, -0.8699],\n",
      "         [-0.3648,  0.9954,  0.3826],\n",
      "         [ 0.8893, -0.9993,  0.8098],\n",
      "         [-0.9993,  1.0000, -0.9992]],\n",
      "\n",
      "        [[-0.9845,  0.9948, -0.8819],\n",
      "         [-0.0183,  0.9062,  0.0102],\n",
      "         [-0.0963, -0.9983,  0.1361],\n",
      "         [-0.9366, -0.1172, -0.9104]],\n",
      "\n",
      "        [[ 0.8107, -0.7307, -0.8231],\n",
      "         [ 0.1578,  0.9942, -0.9813],\n",
      "         [ 0.9942, -0.8544, -0.2598],\n",
      "         [ 0.8986, -0.7957, -0.8728]],\n",
      "\n",
      "        [[ 0.8804,  0.9346,  0.5104],\n",
      "         [ 0.7575, -0.9955,  0.0305],\n",
      "         [ 0.9994, -1.0000, -0.8629],\n",
      "         [-0.9942,  0.9979, -0.9964]],\n",
      "\n",
      "        [[-0.9905,  0.8536, -0.6930],\n",
      "         [ 0.9995, -0.9828,  0.9729],\n",
      "         [-0.9874, -0.5979, -0.8219],\n",
      "         [-0.2618, -0.9999, -0.6171]],\n",
      "\n",
      "        [[ 0.4998,  0.9375,  0.0260],\n",
      "         [ 0.8399,  0.9930, -0.3542],\n",
      "         [-0.5494, -0.0133, -0.3357],\n",
      "         [ 1.0000, -1.0000,  0.7223]],\n",
      "\n",
      "        [[ 0.9901, -0.9520, -0.0819],\n",
      "         [ 0.2246, -0.9049, -0.9810],\n",
      "         [ 0.5959, -0.2689, -0.8820],\n",
      "         [ 0.9380, -0.9939, -0.9721]],\n",
      "\n",
      "        [[ 0.9850, -0.2130, -0.7945],\n",
      "         [-0.0321,  0.9654, -0.9614],\n",
      "         [ 0.7120,  0.9821, -0.4073],\n",
      "         [ 0.9968, -0.9984,  0.4235]]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "#Initialize hiddens for update.\n",
    "hiddens = torch.zeros(batch_size, seq_len, hidden_size)\n",
    "h_t = torch.zeros(batch_size, hidden_size)\n",
    "\n",
    "#Insert your code here\n",
    "for t in range(seq_len):\n",
    "    x_t = inputs[:, t, :]\n",
    "    h_t = torch.tanh(x_t @ U + h_t @ W + b)\n",
    "    hiddens[:, t, :] = h_t\n",
    "    \n",
    "print(hiddens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sC3gaFvLtyUW"
   },
   "source": [
    "(3) In what follows, you need to write the code to compute the logits based on the last hidden state (``[batch_size, hidden_size]``) of hiddens.\n",
    "\n",
    "<div style=\"text-align: right\"><font color=\"red\">[1 mark]</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{logits} = h_t \\cdot V + c$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Psr-vrw8uCMN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.2113,  1.5480,  1.2318],\n",
      "        [-3.0808,  1.1886,  1.8201],\n",
      "        [-0.8212,  0.0486,  1.2706],\n",
      "        [-4.2033,  1.5431,  1.2296],\n",
      "        [-1.5147,  0.4446,  1.9041],\n",
      "        [ 0.3320, -1.0254,  0.8818],\n",
      "        [-0.6590,  0.0403,  1.3935],\n",
      "        [ 0.1636, -0.8414,  0.9678]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "logits = hiddens[:, -1, :] @ V + c\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DEJQQz9uKFb"
   },
   "source": [
    "(4) Write the code to compute the cross-entropy loss by comparing the logits to the labels. You can use PyTorch's built-in loss function.\n",
    "\n",
    "<div style=\"text-align: right\"><font color=\"red\">[1 mark]</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Gyy7lbacuWPR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1817, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Insert your code here\n",
    "import torch.nn as nn\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(logits, random_labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVt-977bumET"
   },
   "source": [
    "(5) Next, you need to do back-propagation to compute the gradients of the loss w.r.t. the model parameters. You can use PyTorch's built-in method to compute the gradients.\n",
    "\n",
    "<div style=\"text-align: right\"><font color=\"red\">[2 marks]</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HSpeFroAu6oO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of U: tensor([[ 0.0475, -0.0114, -0.0843],\n",
      "        [ 0.0852,  0.0128, -0.0570],\n",
      "        [ 0.0134,  0.0183,  0.0690],\n",
      "        [ 0.0211,  0.0727,  0.0090],\n",
      "        [-0.0249,  0.0558,  0.0974]])\n",
      "Gradient of W: tensor([[ 0.0206, -0.0285,  0.0096],\n",
      "        [-0.0557,  0.0531,  0.0788],\n",
      "        [ 0.0623,  0.0184, -0.0770]])\n",
      "Gradient of V: tensor([[-0.0298, -0.0845,  0.1143],\n",
      "        [ 0.0255,  0.2880, -0.3135],\n",
      "        [-0.0691,  0.1421, -0.0729]])\n",
      "Gradient of b: tensor([-0.0819, -0.1359,  0.1229])\n",
      "Gradient of c: tensor([-0.0217, -0.3419,  0.3636])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "\n",
    "print(\"Gradient of U:\", U.grad)\n",
    "print(\"Gradient of W:\", W.grad)\n",
    "print(\"Gradient of V:\", V.grad)\n",
    "print(\"Gradient of b:\", b.grad)\n",
    "print(\"Gradient of c:\", c.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEkHFzOovBx1"
   },
   "source": [
    "(6) Finally, let assume that the learning rate $\\eta = 0.1$, you need to write the code to **manually** update the new model parameters using the SGD manner.\n",
    "\n",
    "<div style=\"text-align: right\"><font color=\"red\">[2 marks]</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2zFSfys8wMqs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Insert your code here\n",
    "learning_rate = 0.1\n",
    "\n",
    "with torch.no_grad():\n",
    "    U -= learning_rate * U.grad\n",
    "    W -= learning_rate * W.grad\n",
    "    V -= learning_rate * V.grad\n",
    "    b -= learning_rate * b.grad\n",
    "    c -= learning_rate * c.grad\n",
    "\n",
    "# Clear the gradient\n",
    "U.grad.zero_()\n",
    "W.grad.zero_()\n",
    "V.grad.zero_()\n",
    "b.grad.zero_()\n",
    "c.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAVPM0BnTdd4"
   },
   "source": [
    "## Section 2: Deep Learning for Sequential Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAgUSME4TsCP"
   },
   "source": [
    "### <font color=\"#0b486b\">Set random seeds</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00nhh4IRUcGX"
   },
   "source": [
    "We start with importing PyTorch and NumPy and setting random seeds for PyTorch and NumPy. You can use any seeds you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "O7XWUry0JXCc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertTokenizer\n",
    "import os\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6ZoWqunmUY7L"
   },
   "outputs": [],
   "source": [
    "def seed_all(seed=1029):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_all(seed=1234)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VU1jS6SUl8q"
   },
   "source": [
    "## <font color=\"#0b486b\">Download and preprocess the data</font>\n",
    "\n",
    "<div style=\"text-align: right\"><font color=\"red; font-weight:bold\"><span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQEzWmZjUulL"
   },
   "source": [
    "The dataset we use for this assignment is a question classification dataset for which the training set consists of $5,500$ questions belonging to 6 coarse question categories including:\n",
    "- abbreviation (ABBR),\n",
    "- entity (ENTY),\n",
    "- description (DESC),\n",
    "- human (HUM),\n",
    "- location (LOC) and\n",
    "- numeric (NUM).\n",
    "\n",
    "In this assignment, we will utilize a subset of this dataset, containing $2,000$ questions for training and validation. We will use 80% of those 2000 questions for trainning and the rest for validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOd49RTpUxxj"
   },
   "source": [
    "Preprocessing data is a crucial initial step in any machine learning or deep learning project. The *TextDataManager* class simplifies the process by providing functionalities to download and preprocess data specifically designed for the subsequent questions in this assignment. It is highly recommended to gain a comprehensive understanding of the class's functionality by **carefully reading** the content provided in the *TextDataManager* class before proceeding to answer the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_C2fuJNzUhha"
   },
   "outputs": [],
   "source": [
    "class DataManager:\n",
    "    \"\"\"\n",
    "    This class manages and preprocesses a simple text dataset for a sentence classification task.\n",
    "\n",
    "    Attributes:\n",
    "        verbose (bool): Controls verbosity for printing information during data processing.\n",
    "        max_sentence_len (int): The maximum length of a sentence in the dataset.\n",
    "        str_questions (list): A list to store the string representations of the questions in the dataset.\n",
    "        str_labels (list): A list to store the string representations of the labels in the dataset.\n",
    "        numeral_labels (list): A list to store the numerical representations of the labels in the dataset.\n",
    "        numeral_data (list): A list to store the numerical representations of the questions in the dataset.\n",
    "        random_state (int): Seed value for random number generation to ensure reproducibility.\n",
    "            Set this value to a specific integer to reproduce the same random sequence every time. Defaults to 6789.\n",
    "        random (np.random.RandomState): Random number generator object initialized with the given random_state.\n",
    "            It is used for various random operations in the class.\n",
    "\n",
    "    Methods:\n",
    "        maybe_download(dir_name, file_name, url, verbose=True):\n",
    "            Downloads a file from a given URL if it does not exist in the specified directory.\n",
    "            The directory and file are created if they do not exist.\n",
    "\n",
    "        read_data(dir_name, file_names):\n",
    "            Reads data from files in a directory, preprocesses it, and computes the maximum sentence length.\n",
    "            Each file is expected to contain rows in the format \"<label>:<question>\".\n",
    "            The labels and questions are stored as string representations.\n",
    "\n",
    "        manipulate_data():\n",
    "            Performs data manipulation by tokenizing, numericalizing, and padding the text data.\n",
    "            The questions are tokenized and converted into numerical sequences using a tokenizer.\n",
    "            The sequences are padded or truncated to the maximum sequence length.\n",
    "\n",
    "        train_valid_test_split(train_ratio=0.9):\n",
    "            Splits the data into training, validation, and test sets based on a given ratio.\n",
    "            The data is randomly shuffled, and the specified ratio is used to determine the size of the training set.\n",
    "            The string questions, numerical data, and numerical labels are split accordingly.\n",
    "            TensorFlow `Dataset` objects are created for the training and validation sets.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, verbose=True, random_state=6789):\n",
    "        self.verbose = verbose\n",
    "        self.max_sentence_len = 0\n",
    "        self.str_questions = list()\n",
    "        self.str_labels = list()\n",
    "        self.numeral_labels = list()\n",
    "        self.maxlen = None\n",
    "        self.numeral_data = list()\n",
    "        self.random_state = random_state\n",
    "        self.random = np.random.RandomState(random_state)\n",
    "\n",
    "    @staticmethod\n",
    "    def maybe_download(dir_name, file_name, url, verbose=True):\n",
    "        if not os.path.exists(dir_name):\n",
    "            os.mkdir(dir_name)\n",
    "        if not os.path.exists(os.path.join(dir_name, file_name)):\n",
    "            urlretrieve(url + file_name, os.path.join(dir_name, file_name))\n",
    "        if verbose:\n",
    "            print(\"Downloaded successfully {}\".format(file_name))\n",
    "\n",
    "    def read_data(self, dir_name, file_names):\n",
    "        self.str_questions = list()\n",
    "        self.str_labels = list()\n",
    "        for file_name in file_names:\n",
    "            file_path= os.path.join(dir_name, file_name)\n",
    "            with open(file_path, \"r\", encoding=\"latin-1\") as f:\n",
    "                for row in f:\n",
    "                    row_str = row.split(\":\")\n",
    "                    label, question = row_str[0], row_str[1]\n",
    "                    question = question.lower()\n",
    "                    self.str_labels.append(label)\n",
    "                    self.str_questions.append(question[0:-1])\n",
    "                    if self.max_sentence_len < len(self.str_questions[-1]):\n",
    "                        self.max_sentence_len = len(self.str_questions[-1])\n",
    "\n",
    "        # turns labels into numbers\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le.fit(self.str_labels)\n",
    "        self.numeral_labels = np.array(le.transform(self.str_labels))\n",
    "        self.str_classes = le.classes_\n",
    "        self.num_classes = len(self.str_classes)\n",
    "        if self.verbose:\n",
    "            print(\"\\nSample questions and corresponding labels... \\n\")\n",
    "            print(self.str_questions[0:5])\n",
    "            print(self.str_labels[0:5])\n",
    "\n",
    "    def manipulate_data(self):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        vocab = self.tokenizer.get_vocab()\n",
    "        self.word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "        self.idx2word = {i:w for w,i in self.word2idx.items()}\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "\n",
    "        token_ids = []\n",
    "        num_seqs = []\n",
    "        for text in self.str_questions:  # iterate over the list of text\n",
    "          text_seqs = self.tokenizer.tokenize(str(text))  # tokenize each text individually\n",
    "          # Convert tokens to IDs\n",
    "          token_ids = self.tokenizer.convert_tokens_to_ids(text_seqs)\n",
    "          # Convert token IDs to a tensor of indices using your word2idx mapping\n",
    "          seq_tensor = torch.LongTensor(token_ids)\n",
    "          num_seqs.append(seq_tensor)  # append the tensor for each sequence\n",
    "\n",
    "        # Pad the sequences and create a tensor\n",
    "        if num_seqs:\n",
    "          self.numeral_data = pad_sequence(num_seqs, batch_first=True)  # Pads to max length of the sequences\n",
    "          self.num_sentences, self.maxlen = self.numeral_data.shape\n",
    "\n",
    "    def train_valid_test_split(self, train_ratio=0.8, test_ratio = 0.1):\n",
    "        train_size = int(self.num_sentences*train_ratio) +1\n",
    "        test_size = int(self.num_sentences*test_ratio) +1\n",
    "        valid_size = self.num_sentences - (train_size + test_size)\n",
    "        data_indices = list(range(self.num_sentences))\n",
    "        random.shuffle(data_indices)\n",
    "        self.train_str_questions = [self.str_questions[i] for i in data_indices[:train_size]]\n",
    "        self.train_numeral_labels = self.numeral_labels[data_indices[:train_size]]\n",
    "        train_set_data = self.numeral_data[data_indices[:train_size]]\n",
    "        train_set_labels = self.numeral_labels[data_indices[:train_size]]\n",
    "        train_set_labels = torch.from_numpy(train_set_labels)\n",
    "        train_set = torch.utils.data.TensorDataset(train_set_data, train_set_labels)\n",
    "        self.test_str_questions = [self.str_questions[i] for i in data_indices[-test_size:]]\n",
    "        self.test_numeral_labels = self.numeral_labels[data_indices[-test_size:]]\n",
    "        test_set_data = self.numeral_data[data_indices[-test_size:]]\n",
    "        test_set_labels = self.numeral_labels[data_indices[-test_size:]]\n",
    "        test_set_labels = torch.from_numpy(test_set_labels)\n",
    "        test_set = torch.utils.data.TensorDataset(test_set_data, test_set_labels)\n",
    "        self.valid_str_questions = [self.str_questions[i] for i in data_indices[train_size:-test_size]]\n",
    "        self.valid_numeral_labels = self.numeral_labels[data_indices[train_size:-test_size]]\n",
    "        valid_set_data = self.numeral_data[data_indices[train_size:-test_size]]\n",
    "        valid_set_labels = self.numeral_labels[data_indices[train_size:-test_size]]\n",
    "        valid_set_labels = torch.from_numpy(valid_set_labels)\n",
    "        valid_set = torch.utils.data.TensorDataset(valid_set_data, valid_set_labels)\n",
    "        self.train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "        self.test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "        self.valid_loader = DataLoader(valid_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3npdESj6Vb_t",
    "outputId": "7822da78-eb83-4989-c737-a75aef97d134"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Downloaded successfully train_2000.label\n",
      "\n",
      "Sample questions and corresponding labels... \n",
      "\n",
      "['manner how did serfdom develop in and then leave russia ?', 'cremat what films featured the character popeye doyle ?', \"manner how can i find a list of celebrities ' real names ?\", 'animal what fowl grabs the spotlight after the chinese year of the monkey ?', 'exp what is the full form of .com ?']\n",
      "['DESC', 'ENTY', 'DESC', 'ENTY', 'ABBR']\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "DataManager.maybe_download(\"data\", \"train_2000.label\", \"http://cogcomp.org/Data/QA/QC/\")\n",
    "\n",
    "dm = DataManager()\n",
    "dm.read_data(\"data/\", [\"train_2000.label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310,
     "referenced_widgets": [
      "2b5d8a6cfdfe4b1e95449279f82761c3",
      "de01c2173d0e440685eb1ac9b884988f",
      "4e32263423444dc8901f4d23cc2b330d",
      "91e59f5a40fc40d983e35386990324d6",
      "37d50405bf0b4e4680f81c082b2e8840",
      "4943de4fa70944cf89e7ca5adfb8273d",
      "7e8d325cf40240fa9b02ede10425ef4e",
      "4761b8cd787c4e63bc46796aca463292",
      "260d4df916764a05a3ada3064296c23b",
      "0db82c9c23d2441d8953e0a47ca01a6b",
      "c34b8a2216574fedbab6a7a596cbc626",
      "f10cf00b28084470ba4dfba1d8a5996d",
      "a72f9d0d378e4c2e9c465d236210b28b",
      "c4f76fdc51b442788f80ab75d79fd969",
      "b9c39c7dff8d4a74a1b7c4f2a686f8c1",
      "c4ae7881130f4e199782e7cc5eb8d8a0",
      "7c9283ea1d5349829123ef15d32aee58",
      "240ea606f4c2434b88965d5a222e4051",
      "2ee0fb07a3974a9cb150fb3accf13c5d",
      "50486104aed747bebdf05efb8a039b7b",
      "37b55f5eea464a77ae5744b86fd294f1",
      "1d3acfbcd78b41e4bd648089e271ed9d",
      "74ac7c2dfc1f4868932a4542b4488604",
      "41cfa051ce84475fbd822b1b1e9ceda4",
      "1f4e711b1e5c480db1261ba94de9efba",
      "a50640383277495291b29f090f4e6b42",
      "8da1e203b84d49f08564f1f2f5fff662",
      "226ea735f94f4ec1bc3e84c98230c170",
      "bf6ee7481825400288abd9abc8585e91",
      "2770be23025f4d5c990b581a171b3160",
      "d17c96976faa4bf2b1b6eac10bb7fc61",
      "dd081a5b24b0459fbb0499041ca6078e",
      "61e6a3b02d574088afee41257a6e72f0",
      "7ae77f5e5f5e4906bc793f3e40edf881",
      "87e8f90800084eab9da3d5abbb8731b8",
      "5aa6842e49594cb98382d9c1639e6f4e",
      "45665a851e7f425bbaf07666f397d97d",
      "dad255bf2c19468ebb366d512e791192",
      "26020e573f5f4503b9030a046ba25d98",
      "502c1c79d2f348eb868194ec493cd110",
      "86f6064c5e174041a5088431df791154",
      "4ab2a6c9c7da44a981cd26448e125c19",
      "df14ee10fe6c44a7a7cd6ba44ce50704",
      "5ab79ed067c0474f95f89938f97a92cf"
     ]
    },
    "id": "EgrYZPmyVj60",
    "outputId": "3609ecf0-4154-4aa4-954d-aee636096ade"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dm.manipulate_data()\n",
    "dm.train_valid_test_split(train_ratio=0.8, test_ratio = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bH-U0sUMVnW-",
    "outputId": "6b3e6b8f-4064-4c9f-d4c0-69d59c109ae0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 36]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for x, y in dm.train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frFf7-ehVvNM"
   },
   "source": [
    "## <font color=\"#0b486b\">Part 1: Using Word2Vect to transform texts to vectors </font>\n",
    "\n",
    "<div style=\"text-align: right\"><font color=\"red; font-weight:bold\">[Total marks for this part: 10 marks]<span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "KZFrETlMVq7P"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JH-f1BJY9bw"
   },
   "source": [
    "#### <font color=\"red\">**Question 1.1**</font>\n",
    "**Write code to download the pretrained model *glove-wiki-gigaword-100*. Note that this model transforms a word in its dictionary to a $100$ dimensional vector.**\n",
    "\n",
    "**Write code for the function *get_word_vector(word, model)* used to transform a word to a vector using the pretrained Word2Vect model *model*. Note that for a word not in the vocabulary of our *word2vect*, you need to return a vector $0$ with 100 dimensions.**\n",
    "\n",
    "<div style=\"text-align: right\"><font color=\"red\">[2 marks]</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b76lh_hZVyac",
    "outputId": "0cd87611-e3db-4e51-c336-9e1b084317f9"
   },
   "outputs": [],
   "source": [
    "word2vect = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector(word, model):\n",
    "    try:\n",
    "        vector = torch.tensor(model[word], dtype=torch.float32)\n",
    "    except KeyError:\n",
    "        vector = torch.zeros(100, dtype=torch.float32)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TjRdyTg2_dN"
   },
   "source": [
    "#### <font color=\"red\">**Question 1.2**</font>\n",
    "\n",
    "**Write the code for the function `get_sentence_vector(sentence, important_score=None, model= None)`. Note that this function will transform a sentence to a 100-dimensional vector using the pretrained model *model*. In addition, the list *important_score* which has the same length as the *sentence* specifies the important scores of the words in the sentence. In your code, you first need to apply *softmax* function over *important_score* to obtain the important weight *important_weight* which forms a probability over the words of the sentence. Furthermore, the final vector of the sentence will be weighted sum of the individual vectors for words and the weights in *important_weight*.**\n",
    "- $important\\_weight = softmax(important\\_score)$.\n",
    "- $final\\_vector= important\\_weight[1]\\times v[1] + important\\_weight[2]\\times v[2] + ...+ important\\_weight[T]\\times v[T]$ where $T$ is the length of the sentence and $v[i]$ is the vector representation of the $i-th$  word in this sentence.\n",
    "\n",
    "**Note that if `important_score=None` is set by default, your function should return the average of all representation vectors corresponding to set `important_score=[1,1,...,1]`.**\n",
    "\n",
    "<div style=\"text-align: right\"><font color=\"red\">[2 marks]</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "MaPs4Hqa2_30"
   },
   "outputs": [],
   "source": [
    "def get_sentence_vector(sentence, important_score=None, model= None):\n",
    "    words = sentence.split()\n",
    "    word_vectors = [get_word_vector(word, model) for word in words]\n",
    "    if important_score is None:\n",
    "        important_score = [1] * len(words)\n",
    "    important_score = torch.tensor(important_score, dtype=torch.float32)\n",
    "    important_weight = torch.softmax(important_score, dim=0)\n",
    "    word_vectors = torch.stack(word_vectors)\n",
    "    weighted_vectors = word_vectors * important_weight.unsqueeze(1)\n",
    "    feature_vector = torch.sum(weighted_vectors, dim=0)\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tu63GJ2c3IgZ"
   },
   "source": [
    "#### <font color=\"red\">**Question 1.3**</font>\n",
    "\n",
    "**Write code to transform questions in *dm.train_str_questions* and *dm.valid_str_questions* to feature vectors. Note that after running the following cells, you must have $X\\_train$ and $X\\_valid$ which are two NumPy arrays of the feature vectors and $y\\_train$ and $y\\_valid$ which are two arrays of numeric labels (Hint: *dm.train_numeral_labels* and *dm.valid_numeral_labels*). You can add more lines to the following cells if necessary. In addition, you should decide the *important_score* by yourself. For example, the 1st score is 1, the 2nd score is decayed by 0.9, the 3rd is decayed by 0.9, and so on.**\n",
    "\n",
    "<div style=\"text-align: right\"><font color=\"red\">[2 marks]</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_score(num_words):\n",
    "    return [random.uniform(0, 1) for _ in range(num_words)]\n",
    "\n",
    "important_scores = [create_random_score(len(sentence.split())) for sentence in dm.train_str_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5SCsWIdZ3I4S",
    "outputId": "4691df38-1e88-47cf-c392-378a0086dfc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform training set to feature vectors...\n"
     ]
    }
   ],
   "source": [
    "print(\"Transform training set to feature vectors...\")\n",
    "X_train = [get_sentence_vector(sentence, important_score, word2vect) \n",
    "           for important_score, sentence in list(zip(important_scores, dm.train_str_questions))]\n",
    "y_train = dm.train_numeral_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_scores = [create_random_score(len(sentence.split())) for sentence in dm.valid_str_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2odznptZ3Nf_",
    "outputId": "d007af2d-bbef-436a-ee62-7940a83fdf5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform validation set to feature vectors...\n"
     ]
    }
   ],
   "source": [
    "print(\"Transform validation set to feature vectors...\")\n",
    "X_valid = [get_sentence_vector(sentence, important_score, word2vect) \n",
    "           for important_score, sentence in list(zip(important_scores, dm.valid_str_questions))]\n",
    "y_valid = dm.valid_numeral_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOp_gy3d3Reh"
   },
   "source": [
    "#### <font color=\"red\">**Question 1.4**</font>\n",
    "\n",
    "**It is now to use *MinMaxScaler(feature_range=(-1,1))* in scikit-learn to scale both training and validation sets to the range $(-1,1)$.**\n",
    "\n",
    "<div style=\"text-align: right\"><font color=\"red\">[2 marks]</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1601, 100) (198, 100)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "print(X_train.shape, X_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NcvKQyHO3ZJx"
   },
   "source": [
    "#### <font color=\"red\">**Question 1.5**</font>\n",
    "**Train a Logistic Regression model on the training set and then evaluate on the validation set.** You can use any classification metrics in `sklearn` for evaluation.\n",
    "<div style=\"text-align: right\"><font color=\"red\">[2 marks]</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "IQYE_rz-3b25"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(max_iter=1000)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ucJWNHMO3gXF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 81.82%\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "y_pred = model.predict(X_valid)\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_valid, y_pred)\n",
    "print(f'Validation Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZs9Y0rl7gBH"
   },
   "source": [
    "We now declare the `BaseTrainer` class, which will be used later to train the subsequent deep learning models for text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "yXlNQvGn7OEb"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class BaseTrainer:\n",
    "    def __init__(self, model, criterion, optimizer, train_loader, val_loader):\n",
    "        self.model = model\n",
    "        self.criterion = criterion  #the loss function\n",
    "        self.optimizer = optimizer  #the optimizer\n",
    "        self.train_loader = train_loader  #the train loader\n",
    "        self.val_loader = val_loader  #the valid loader\n",
    "\n",
    "    #the function to train the model in many epochs\n",
    "    def fit(self, num_epochs):\n",
    "        self.num_batches = len(self.train_loader)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "            train_loss, train_accuracy = self.train_one_epoch()\n",
    "            val_loss, val_accuracy = self.validate_one_epoch()\n",
    "            print(\n",
    "                f'{self.num_batches}/{self.num_batches} - train_loss: {train_loss:.4f} - train_accuracy: {train_accuracy*100:.4f}% \\\n",
    "                - val_loss: {val_loss:.4f} - val_accuracy: {val_accuracy*100:.4f}%')\n",
    "\n",
    "    #train in one epoch, return the train_acc, train_loss\n",
    "    def train_one_epoch(self):\n",
    "        self.model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        for i, data in enumerate(self.train_loader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        train_accuracy = correct / total\n",
    "        train_loss = running_loss / self.num_batches\n",
    "        return train_loss, train_accuracy\n",
    "\n",
    "    #evaluate on a loader and return the loss and accuracy\n",
    "    def evaluate(self, loader):\n",
    "        self.model.eval()\n",
    "        loss, correct, total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for data in loader:\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = correct / total\n",
    "        loss = loss / len(self.val_loader)\n",
    "        return loss, accuracy\n",
    "\n",
    "    #return the val_acc, val_loss, be called at the end of each epoch\n",
    "    def validate_one_epoch(self):\n",
    "      val_loss, val_accuracy = self.evaluate(self.val_loader)\n",
    "      return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GWOQN_S7p4V"
   },
   "source": [
    "## <font color=\"#0b486b\">Part 2: Text CNN for sequence modeling and neural embedding </font>\n",
    "\n",
    "<div style=\"text-align: right\"><font color=\"red; font-weight:bold\">[Total marks for this part: 10 marks]<span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJub3Fwm7vC7"
   },
   "source": [
    "**In what follows, you are required to complete the code for Text CNN for sentence classification. The paper of Text CNN can be found at this [link](https://www.aclweb.org/anthology/D14-1181.pdf). Here is the description of the Text CNN that you need to construct.**\n",
    "- There are three attributes (properties or instance variables): *embed_size, state_size, data_manager*.\n",
    "  - `embed_size`: the dimension of the vector space for which the words are embedded to using the embedding matrix.\n",
    "  - `state_size`: the number of filters used in *Conv1D* (reference [here](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html)).\n",
    "  - `data_manager`: the data manager to store information of the dataset.\n",
    "- The detail of the computational process is as follows:\n",
    "  - Given input $x$, we embed $x$ using the embedding matrix to obtain an $3D$ tensor $[batch\\_size, seq\\_len, embed\\_size]$ as $e$.\n",
    "  - We feed $e$ to three *Conv1D* layers, each of which has $state\\_size$ filters, activation= $relu$, and $kernel\\_size= 3, 5, 7$ respectively to obtain $h1, h2, h3$. Note that each $h1, h2, h3$ is a 3D tensor with the shape $[batch\\_size, state\\_size, output\\_size]$. Moreover, you need to apply *Conv1D* to the $seq\\_len$ dimension.\n",
    "  - We then apply *GlobalMaxPool1D()* (reference [here](https://pytorch.org/docs/stable/generated/torch.nn.functional.max_pool1d.html#torch.nn.functional.max_pool1d)) over $h1, h2, h3$ to obtain 2D tensors stored in $h1, h2, h3$ again.\n",
    "  - We then concatenate three 2D tensors $h1, h2, h3$ to obtain $h$ with the shape $\\left[batch\\_size, 3\\times state\\_size\\right]$. Note that you need to specify the axis to concatenate.\n",
    "  - We finally build up one dense layer $\\left[3\\times state\\_size, num\\_classes\\right]$  on the top of $h$ for classification.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "lEnG-BGJ239k"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#You can modify the code if you want but need to keep the skeleton\n",
    "class TextCNN(torch.nn.Module):\n",
    "    def __init__(self, embed_size=128, state_size=16, data_manager=None):\n",
    "        super().__init__()\n",
    "        self.data_manager = data_manager\n",
    "        self.embed_size = embed_size\n",
    "        self.state_size = state_size\n",
    "        #declare the necessary layers here\n",
    "        self.embed = nn.Embedding(self.data_manager.vocab_size, self.embed_size)\n",
    "        self.conv1d_1 = nn.Conv1d(in_channels=self.embed_size, out_channels=self.state_size, kernel_size=3)\n",
    "        self.conv1d_2 = nn.Conv1d(in_channels=self.embed_size, out_channels=self.state_size, kernel_size=5)\n",
    "        self.conv1d_3 = nn.Conv1d(in_channels=self.embed_size, out_channels=self.state_size, kernel_size=7)\n",
    "        self.fc = nn.Linear(state_size*3, self.data_manager.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e = self.embed(x) # [batch_size, seq_len, embed_size]\n",
    "        #permute x before applying Conv1D\n",
    "        e = e.permute(0,2,1) # [batch_size, embed_size, seq_len]\n",
    "\n",
    "        #applying Conv1D\n",
    "        h1 = F.relu(self.conv1d_1(e)) \n",
    "        h2 = F.relu(self.conv1d_2(e))\n",
    "        h3 = F.relu(self.conv1d_3(e))\n",
    "\n",
    "        #apply GlobalMaxPool\n",
    "        h1 = F.max_pool1d(h1, h1.size(2)).squeeze(2) # [batch_size, state_size]\n",
    "        h2 = F.max_pool1d(h2, h2.size(2)).squeeze(2) # [batch_size, state_size]\n",
    "        h3 = F.max_pool1d(h3, h3.size(2)).squeeze(2) # [batch_size, state_size]\n",
    "\n",
    "        h = torch.cat([h1, h2, h3], dim=1) # [batch_size, state_size * 3]\n",
    "        h = self.fc(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-PN6KmIFw6K"
   },
   "source": [
    "We declare `text_cnn` and train on several epochs (e.g., `50 epochs`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IrL5oM82BR3_",
    "outputId": "494f66d3-98b3-4768-8d82-46e32f1134aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "26/26 - train_loss: 1.6139 - train_accuracy: 36.8520%                 - val_loss: 0.5991 - val_accuracy: 71.7172%\n",
      "Epoch 2/50\n",
      "26/26 - train_loss: 0.8245 - train_accuracy: 80.3248%                 - val_loss: 0.3028 - val_accuracy: 80.3030%\n",
      "Epoch 3/50\n",
      "26/26 - train_loss: 0.4867 - train_accuracy: 89.7564%                 - val_loss: 0.1956 - val_accuracy: 90.4040%\n",
      "Epoch 4/50\n",
      "26/26 - train_loss: 0.2666 - train_accuracy: 94.4410%                 - val_loss: 0.1611 - val_accuracy: 90.9091%\n",
      "Epoch 5/50\n",
      "26/26 - train_loss: 0.1737 - train_accuracy: 97.0643%                 - val_loss: 0.1311 - val_accuracy: 92.9293%\n",
      "Epoch 6/50\n",
      "26/26 - train_loss: 0.1166 - train_accuracy: 98.8757%                 - val_loss: 0.1156 - val_accuracy: 93.4343%\n",
      "Epoch 7/50\n",
      "26/26 - train_loss: 0.0937 - train_accuracy: 99.5628%                 - val_loss: 0.0813 - val_accuracy: 93.9394%\n",
      "Epoch 8/50\n",
      "26/26 - train_loss: 0.0773 - train_accuracy: 98.5009%                 - val_loss: 0.0964 - val_accuracy: 94.9495%\n",
      "Epoch 9/50\n",
      "26/26 - train_loss: 0.0471 - train_accuracy: 99.8751%                 - val_loss: 0.0804 - val_accuracy: 93.9394%\n",
      "Epoch 10/50\n",
      "26/26 - train_loss: 0.0364 - train_accuracy: 100.0000%                 - val_loss: 0.0670 - val_accuracy: 93.9394%\n",
      "Epoch 11/50\n",
      "26/26 - train_loss: 0.0301 - train_accuracy: 100.0000%                 - val_loss: 0.0722 - val_accuracy: 94.4444%\n",
      "Epoch 12/50\n",
      "26/26 - train_loss: 0.0241 - train_accuracy: 100.0000%                 - val_loss: 0.0625 - val_accuracy: 94.9495%\n",
      "Epoch 13/50\n",
      "26/26 - train_loss: 0.0193 - train_accuracy: 100.0000%                 - val_loss: 0.0480 - val_accuracy: 94.9495%\n",
      "Epoch 14/50\n",
      "26/26 - train_loss: 0.0164 - train_accuracy: 100.0000%                 - val_loss: 0.0545 - val_accuracy: 95.4545%\n",
      "Epoch 15/50\n",
      "26/26 - train_loss: 0.0139 - train_accuracy: 100.0000%                 - val_loss: 0.0430 - val_accuracy: 95.4545%\n",
      "Epoch 16/50\n",
      "26/26 - train_loss: 0.0119 - train_accuracy: 100.0000%                 - val_loss: 0.0452 - val_accuracy: 95.9596%\n",
      "Epoch 17/50\n",
      "26/26 - train_loss: 0.0103 - train_accuracy: 100.0000%                 - val_loss: 0.0380 - val_accuracy: 95.4545%\n",
      "Epoch 18/50\n",
      "26/26 - train_loss: 0.0091 - train_accuracy: 100.0000%                 - val_loss: 0.0358 - val_accuracy: 95.4545%\n",
      "Epoch 19/50\n",
      "26/26 - train_loss: 0.0079 - train_accuracy: 100.0000%                 - val_loss: 0.0342 - val_accuracy: 95.9596%\n",
      "Epoch 20/50\n",
      "26/26 - train_loss: 0.0074 - train_accuracy: 100.0000%                 - val_loss: 0.0311 - val_accuracy: 95.9596%\n",
      "Epoch 21/50\n",
      "26/26 - train_loss: 0.0065 - train_accuracy: 100.0000%                 - val_loss: 0.0280 - val_accuracy: 95.9596%\n",
      "Epoch 22/50\n",
      "26/26 - train_loss: 0.0057 - train_accuracy: 100.0000%                 - val_loss: 0.0275 - val_accuracy: 95.9596%\n",
      "Epoch 23/50\n",
      "26/26 - train_loss: 0.0055 - train_accuracy: 100.0000%                 - val_loss: 0.0254 - val_accuracy: 95.9596%\n",
      "Epoch 24/50\n",
      "26/26 - train_loss: 0.0052 - train_accuracy: 100.0000%                 - val_loss: 0.0248 - val_accuracy: 95.9596%\n",
      "Epoch 25/50\n",
      "26/26 - train_loss: 0.0045 - train_accuracy: 100.0000%                 - val_loss: 0.0191 - val_accuracy: 95.9596%\n",
      "Epoch 26/50\n",
      "26/26 - train_loss: 0.0042 - train_accuracy: 100.0000%                 - val_loss: 0.0180 - val_accuracy: 95.9596%\n",
      "Epoch 27/50\n",
      "26/26 - train_loss: 0.0038 - train_accuracy: 100.0000%                 - val_loss: 0.0180 - val_accuracy: 95.9596%\n",
      "Epoch 28/50\n",
      "26/26 - train_loss: 0.0033 - train_accuracy: 100.0000%                 - val_loss: 0.0174 - val_accuracy: 95.9596%\n",
      "Epoch 29/50\n",
      "26/26 - train_loss: 0.0031 - train_accuracy: 100.0000%                 - val_loss: 0.0169 - val_accuracy: 95.9596%\n",
      "Epoch 30/50\n",
      "26/26 - train_loss: 0.0029 - train_accuracy: 100.0000%                 - val_loss: 0.0157 - val_accuracy: 96.4646%\n",
      "Epoch 31/50\n",
      "26/26 - train_loss: 0.0026 - train_accuracy: 100.0000%                 - val_loss: 0.0154 - val_accuracy: 96.4646%\n",
      "Epoch 32/50\n",
      "26/26 - train_loss: 0.0025 - train_accuracy: 100.0000%                 - val_loss: 0.0142 - val_accuracy: 96.4646%\n",
      "Epoch 33/50\n",
      "26/26 - train_loss: 0.0023 - train_accuracy: 100.0000%                 - val_loss: 0.0134 - val_accuracy: 96.4646%\n",
      "Epoch 34/50\n",
      "26/26 - train_loss: 0.0022 - train_accuracy: 100.0000%                 - val_loss: 0.0124 - val_accuracy: 95.9596%\n",
      "Epoch 35/50\n",
      "26/26 - train_loss: 0.0020 - train_accuracy: 100.0000%                 - val_loss: 0.0115 - val_accuracy: 96.4646%\n",
      "Epoch 36/50\n",
      "26/26 - train_loss: 0.0019 - train_accuracy: 100.0000%                 - val_loss: 0.0112 - val_accuracy: 96.4646%\n",
      "Epoch 37/50\n",
      "26/26 - train_loss: 0.0018 - train_accuracy: 100.0000%                 - val_loss: 0.0107 - val_accuracy: 96.4646%\n",
      "Epoch 38/50\n",
      "26/26 - train_loss: 0.0017 - train_accuracy: 100.0000%                 - val_loss: 0.0102 - val_accuracy: 96.4646%\n",
      "Epoch 39/50\n",
      "26/26 - train_loss: 0.0016 - train_accuracy: 100.0000%                 - val_loss: 0.0100 - val_accuracy: 96.4646%\n",
      "Epoch 40/50\n",
      "26/26 - train_loss: 0.0015 - train_accuracy: 100.0000%                 - val_loss: 0.0095 - val_accuracy: 96.4646%\n",
      "Epoch 41/50\n",
      "26/26 - train_loss: 0.0015 - train_accuracy: 100.0000%                 - val_loss: 0.0090 - val_accuracy: 96.4646%\n",
      "Epoch 42/50\n",
      "26/26 - train_loss: 0.0014 - train_accuracy: 100.0000%                 - val_loss: 0.0087 - val_accuracy: 96.4646%\n",
      "Epoch 43/50\n",
      "26/26 - train_loss: 0.0013 - train_accuracy: 100.0000%                 - val_loss: 0.0084 - val_accuracy: 96.4646%\n",
      "Epoch 44/50\n",
      "26/26 - train_loss: 0.0014 - train_accuracy: 100.0000%                 - val_loss: 0.0081 - val_accuracy: 96.4646%\n",
      "Epoch 45/50\n",
      "26/26 - train_loss: 0.0012 - train_accuracy: 100.0000%                 - val_loss: 0.0076 - val_accuracy: 95.9596%\n",
      "Epoch 46/50\n",
      "26/26 - train_loss: 0.0011 - train_accuracy: 100.0000%                 - val_loss: 0.0077 - val_accuracy: 96.4646%\n",
      "Epoch 47/50\n",
      "26/26 - train_loss: 0.0010 - train_accuracy: 100.0000%                 - val_loss: 0.0073 - val_accuracy: 96.4646%\n",
      "Epoch 48/50\n",
      "26/26 - train_loss: 0.0012 - train_accuracy: 100.0000%                 - val_loss: 0.0074 - val_accuracy: 96.4646%\n",
      "Epoch 49/50\n",
      "26/26 - train_loss: 0.0010 - train_accuracy: 100.0000%                 - val_loss: 0.0079 - val_accuracy: 96.4646%\n",
      "Epoch 50/50\n",
      "26/26 - train_loss: 0.0009 - train_accuracy: 100.0000%                 - val_loss: 0.0064 - val_accuracy: 96.4646%\n"
     ]
    }
   ],
   "source": [
    "text_cnn = TextCNN(data_manager=dm).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(text_cnn.parameters(), lr=0.001)\n",
    "trainer = BaseTrainer(model=text_cnn, criterion=criterion, optimizer=optimizer, train_loader=dm.train_loader, val_loader=dm.valid_loader)\n",
    "trainer.fit(num_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrjO5iT6F_Li"
   },
   "source": [
    "We evaluate the trained model on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CnHiWCl7BtJC",
    "outputId": "ba7b7eba-d589-4b2e-fd10-57b7127ca61f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.2677 - test_accuracy: 95.5224%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = trainer.evaluate(dm.test_loader)\n",
    "print(f'test_loss: {test_loss:.4f} - test_accuracy: {test_acc*100:.4f}%')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
